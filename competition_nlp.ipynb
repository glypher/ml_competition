{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"competition_nlp.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WTOa2S0ByXHS"},"source":["#### Romanian Sentence Classification\n","\n","Mihai Matei [Data Science]\n","\n"]},{"cell_type":"code","metadata":{"id":"BbuObLCnyXHS"},"source":["import sys\n","import os\n","import warnings\n","import shutil\n","from datetime import datetime\n","import json\n","DATA_DIR='data_nlp'\n","OUTPUT_DIR='output_nlp'\n","\n","def mount_gdrive():\n","    global DATA_DIR\n","    global OUTPUT_DIR\n","    # Import the library and kaggle config files from gdrive\n","    GDRIVE_PATH='/content/drive/MyDrive/RESEARCH/'\n","    if 'google.colab' in sys.modules:\n","        from google.colab import drive\n","        import shutil\n","        drive.mount('/content/drive/')\n","        sys.path.append(GDRIVE_PATH)\n","        os.makedirs('/root/.kaggle/', exist_ok=True)\n","        shutil.copyfile(GDRIVE_PATH + 'kaggle.json', '/root/.kaggle/kaggle.json')\n","        !chmod 600 '/root/.kaggle/kaggle.json'\n","        DATA_DIR = os.path.join(GDRIVE_PATH, DATA_DIR)\n","        OUTPUT_DIR = os.path.join(GDRIVE_PATH, OUTPUT_DIR)\n","\n","def install_modules():\n","    !pip install --quiet kaggle\n","    !pip install --quiet pandas\n","    !pip install --quiet scikit-learn\n","    !pip install --quiet scikit-image\n","    !pip install --upgrade --quiet tensorflow\n","    !pip install --quiet randomcolor\n","    !pip install --quiet matplotlib\n","    !pip install --quiet Pillow\n","    !pip install --quiet seaborn\n","    !pip install --quiet tf-models-official\n","    !pip install --quiet snowballstemmer\n","    !pip install --quiet nltk\n","    if 'google.colab' in sys.modules:\n","        # currently the windows wheel depends on tf 2.4rc3\n","        !pip install --quiet tensorflow-text\n","\n","mount_gdrive()\n","install_modules()\n","os.makedirs(DATA_DIR, exist_ok=True)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs('model_checkpoint', exist_ok=True)\n","\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('punkt')\n","import snowballstemmer\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","\n","#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","print(tf.python.platform.build_info.build_info)\n","\n","TRAIN_EPOCHS=30\n","\n","NO_CLASSES=10\n","MAX_LENGTH=128\n","BATCH_SIZE=32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuMq5BGayXHT"},"source":["!git clone 'https://glypher:886cd6845d8a78081ae9cd4b6c259722f3b7ca3e@github.com/glypher/matmih.git'\n","!git -C matmih pull\n","\n","#%reload_ext autoreload\n","#%autoreload 2\n","import matmih as mm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Av6i-WRMyXHT"},"source":["#### Download the database\n","Create a kaggle account and an API token \n","https://github.com/Kaggle/kaggle-api/blob/master/README.md"]},{"cell_type":"code","metadata":{"id":"09eicgTcyXHT"},"source":["# Download the kaggle competition database\n","if not os.path.isfile(f'{DATA_DIR}/train.txt'):\n","    import kaggle, zipfile\n","    kaggle.api.authenticate()\n","    kaggle.api.competition_download_files('dl-2020-unibuc-nlp', path=f'./{DATA_DIR}')\n","    with zipfile.ZipFile(f'{DATA_DIR}/dl-2020-unibuc-nlp.zip') as z:\n","        z.extractall(f'{DATA_DIR}')\n","    os.remove(f'{DATA_DIR}/dl-2020-unibuc-nlp.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8yMqPlSyXHT"},"source":["#### Romanian sentence data set\n"]},{"cell_type":"code","metadata":{"id":"ajwfExBdyXHT"},"source":["def show_random_classes(df):\n","    import random\n","    from datetime import datetime\n","    random.seed(datetime.now())\n","    for cl in df['target'].cat.categories:\n","        class_df = df[df['target'] == cl]\n","        idx = random.randrange(len(class_df))\n","        print(f\"Class: {class_df['target'].iloc[idx]} Sentence: {class_df['text'].iloc[idx]}\")\n","\n","def load_data(data_type, test=False):\n","    cols = ['sid', 'target', 'text'] if not test else ['sid', 'text', 'target']\n","    df = pd.read_csv(f\"{DATA_DIR}/{data_type}.txt\", delimiter=',', names=cols)\n","    df.dataframeName = data_type\n","    \n","    df['text'] = df['text'].astype('string')\n","    df['sid'] = df['sid'].astype('int')\n","    if not test:\n","        df['target'] = df['target'].astype('int').apply(lambda x: x-1).astype('category')\n","\n","    return df\n","\n","\n","trainDF = load_data('train')\n","valDF = load_data('vaidation')\n","testDF = load_data('test', test=True)\n","mm.PlotBuilder().create_subplots(1,2, (13, 6)).create_histograms(\n","    [trainDF['target'], valDF['target']],\n","    ['Class histogram TRAIN', 'Class histogram VALIDATION']).show()\n","\n","show_random_classes(trainDF)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saS9hMsfNY0W"},"source":["def plot_text_length(df, title, pb=None, per_class=True, no_bins=10):\n","    data = []\n","    titles = []\n","    if per_class:\n","        for id in range(0, NO_CLASSES):\n","            df_class = df[df['target'] == id]\n","            lengths = np.array(df_class['text'].str.len())\n","            data.append((lengths, 'auto'))\n","    \n","            titles.append(f'{title} class {id}')\n","    else:\n","        lengths = np.array(df['text'].str.len())\n","        data = [(lengths, 'auto')]\n","        titles = [title]\n","\n","    if pb is None:\n","        pb = mm.PlotBuilder()\n","    \n","    pb.create_histograms(data, titles)\n","\n","print(f'BEFORE Filtering TRAIN={len(trainDF)} VAL={len(valDF)} TEST={len(testDF)}')\n","# filter the train and validation set for nan text\n","trainDF['text'] = trainDF['text'].replace(pd.NA, \"\")\n","trainDF['text'] = trainDF['text'].replace(\"\", pd.NA)\n","trainDF = trainDF.dropna(inplace=False)\n","\n","valDF['text'] = valDF['text'].replace(pd.NA, \"\")\n","valDF['text'] = valDF['text'].replace(\"\", pd.NA)\n","valDF = valDF.dropna(inplace=False)\n","\n","testDF['text'] = testDF['text'].replace(pd.NA, \"\")\n","testDF['text'] = testDF['text'].replace(\"\", \"invalid\")\n","print(f'AFTER Filtering TRAIN={len(trainDF)} VAL={len(valDF)} TEST={len(testDF)}')\n","\n","pb = mm.PlotBuilder().create_subplots(1, 3, (20, 5))\n","plot_text_length(trainDF, title=\"TRAIN\", pb=pb, per_class=False)\n","plot_text_length(valDF, title=\"VALIDATION\", pb=pb, per_class=False)\n","plot_text_length(testDF, title=\"TEST\", pb=pb, per_class=False)\n","pb.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKZdsvzuKJyU"},"source":["#word_lengths = []\n","#words_split_lengths = []\n","def split_dataframe(df, functor):\n","    newDF = pd.concat([pd.Series(row['sid'], functor(row['text']))\n","                       for _, row in df.iterrows()]).reset_index()\n","    newDF = newDF.rename(columns={'index':\"text\", 0:\"sid\"})\n","    newDF = newDF.merge(df[['target', 'sid']], on=\"sid\", how = 'inner')\n","    return newDF\n","\n","def split_sentence(s, max_length=64, delta=0):\n","    data = []\n","    words = [w for w in nltk.word_tokenize(s)]\n","    #if len(words) < 5:\n","    #    print(words)\n","    #word_lengths.append(len(words))\n","    while True:\n","        data.append(''.join([w+' ' if w.isalnum() else w for w in words[:max_length]]))\n","        if (len(words) < max_length):\n","            break\n","  \n","        words = words[max_length-delta:]\n","\n","    return data\n","\n","\n","def split_train(df, max_length, delta):\n","    dfsplit = split_dataframe(df, nltk.sent_tokenize)\n","    dfsplit = split_dataframe(dfsplit, split_sentence)\n","    return dfsplit\n","\n","\n","#split into sentences\n","trainDFsplit = split_train(trainDF, 128, 0)\n","valDFsplit = split_train(valDF, 128, 0)\n","testDFsplit = split_train(testDF, 128, 0)\n","\n","pb = mm.PlotBuilder().create_subplots(1, 3, (20, 5))\n","plot_text_length(trainDFsplit, title=\"TRAIN\", pb=pb, per_class=False)\n","plot_text_length(valDFsplit, title=\"VALIDATION\", pb=pb, per_class=False)\n","plot_text_length(testDFsplit, title=\"TEST\", pb=pb, per_class=False)\n","pb.show()\n","\n","#mm.PlotBuilder().create_subplots(1, 3, (30, 5)).create_histograms(\n","#    [(word_lengths, 'auto'), (words_split_lengths, 'auto')], ['All word length', 'All word splits']).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxqh3aLAVwW5"},"source":["#stemmer = snowballstemmer.RomanianStemmer()\n","\n","#trainDFsplit['text'] = trainDFsplit['text'].apply(lambda text: ' '.join([stemmer.stemWord(w) for w in nltk.word_tokenize(text)]))\n","#valDFsplit['text'] = valDFsplit['text'].apply(lambda text: ' '.join([stemmer.stemWord(w) for w in nltk.word_tokenize(text)]))\n","#testDFsplit['text'] = testDFsplit['text'].apply(lambda text: ' '.join([stemmer.stemWord(w) for w in nltk.word_tokenize(text)]))\n","\n","#pb = mm.PlotBuilder().create_subplots(1, 3, (16, 6))\n","#plot_text_length(trainDFsplit, title=\"TRAIN\", pb=pb, per_class=False)\n","#plot_text_length(valDFsplit, title=\"VALIDATION\", pb=pb, per_class=False)\n","#plot_text_length(testDFsplit, title=\"TEST\", pb=pb, per_class=False)\n","#pb.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6RQreagI8Mc"},"source":["### TF Datasets"]},{"cell_type":"code","metadata":{"id":"J31LEAcaDiZK"},"source":["class TestDataset(tf.data.Dataset):\n","    def __new__(cls, df):\n","        def _generator_data():\n","            for _, row in df.iterrows():\n","                yield (tf.cast(row['text'], tf.string))\n","                                \n","        return tf.data.Dataset.from_generator(_generator_data, (tf.string),\n","                                              (tf.TensorShape([]))).batch(\n","            BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","class TrainDataset(tf.data.Dataset):\n","    def __new__(cls, df):\n","        def _generator_data():\n","            for _, row in df.iterrows():\n","                yield (tf.cast(row['text'], tf.string), tf.cast(row['target'], tf.int32))\n","                                \n","        return tf.data.Dataset.from_generator(_generator_data, (tf.string, tf.int32),\n","                                              (tf.TensorShape([]), tf.TensorShape([]))).shuffle(\n","            3000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","def get_class_weights(df):\n","    import sklearn\n","    weights=sklearn.utils.class_weight.compute_class_weight('balanced',\n","                                                            classes=range(NO_CLASSES),\n","                                                            y=df['target'])\n","    return {i:weights[i] for i in range(NO_CLASSES)}\n","\n","class_weights = get_class_weights(trainDFsplit)\n","print(f\"Class weights {class_weights}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJTMqxdzyXHT"},"source":["### Bert Model\n","\n","* dropoutRate - dropout layer percentage\n","* trainEpochs - number of epochs to train\n","* class_weights - if the target class distribution should be taken into account when computing the loss\n","* optimizer - the Keras optimizer to use (will also include parameters as learning rate)"]},{"cell_type":"code","metadata":{"id":"UCC-xlrKyXHT"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import official\n","\n","#BERT_FOLDER='bert/multi_cased_L-12_H-768_A-12'\n","#BERT_FOLDER='bert/bert_en_cased_L-12_H-768_A-12_3'\n","\n","class BertModel(mm.TensorModel):\n","    def __init__(self, **hyper_params):\n","        self._hyper_params = hyper_params.copy()\n","\n","        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","        layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1\",\n","                               name=\"BERT_preprocessing\")(text_input)\n","\n","        layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\",\n","                               trainable=True, name=\"BERT_Encoder\")(layer)\n","\n","        if hyper_params['pooled_size'] == 'all':\n","            layer = layer['pooled_output']\n","        elif hyper_params['pooled_size'] == 'max_words':\n","            layer = layer[\"sequence_output\"][:, 0:hyper_params['max_words'], :]\n","            layer = tf.math.reduce_mean(layer, axis=1)\n","        else:\n","            layer = layer[\"sequence_output\"][:, 0:hyper_params['pooled_size'], :]\n","            layer = tf.math.reduce_mean(layer, axis=1)\n","\n","        layer = tf.keras.layers.Dense(128)(layer)\n","        layer = tf.keras.layers.BatchNormalization()(layer)\n","        layer = tf.keras.layers.Activation('relu')(layer)\n","\n","        layer = tf.keras.layers.Dense(256)(layer)\n","        layer = tf.keras.layers.BatchNormalization()(layer)\n","        layer = tf.keras.layers.Activation('relu')(layer)\n","\n","        layer = tf.keras.layers.Dropout(hyper_params.get('dropoutRate'))(layer)\n","\n","        layer = tf.keras.layers.Dense(NO_CLASSES)(layer)\n","        layer = tf.keras.layers.Activation('softmax')(layer)\n","\n","        super(BertModel, self).__init__(tf.keras.Model(text_input, layer),\n","                                        checkpoint=True)\n","\n","        self._train_epochs = hyper_params.get('trainEpochs')\n","        self._optimizer = hyper_params.get('optimizer')\n","\n","        # compile the model and initialize the weights\n","        self._model.compile(\n","             optimizer=self._optimizer(),\n","             loss='sparse_categorical_crossentropy',\n","             metrics=['accuracy'])\n","\n","    def train(self, data, logTensorBoard=False):\n","        train_ds = data[0]\n","        val_ds = data[1]\n","\n","        # save the weights of the best epoch to disk\n","        callbacks = []\n","        callbacks += [tf.keras.callbacks.ModelCheckpoint(self.best_weights_path, monitor='val_accuracy',\n","                                                         verbose=0, save_best_only=True,\n","                                                         save_weights_only=True, mode='auto')]\n","        if logTensorBoard:\n","            callbacks += [tf.keras.callbacks.TensorBoard(mm.TensorBoard.get_log_dir(), histogram_freq=1)]\n","\n","        if val_ds is not None:\n","            callbacks += [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n","                                                           min_delta=0.001, mode='max', patience=5)]\n","            #callbacks += [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","            #                                                   patience=2, min_lr=0.001)]\n","\n","        \n","        train_ds = split_train(train_ds, self._hyper_params['max_words'], self._hyper_params['delta'])\n","        train_ds = TrainDataset(train_ds)\n","        if val_ds is not None:\n","            val_ds = split_train(val_ds, self._hyper_params['max_words'], self._hyper_params['delta'])\n","            val_ds = TrainDataset(val_ds)\n","\n","        history = self._model.fit(train_ds,\n","                                  validation_data=val_ds,\n","                                  epochs=self._train_epochs,\n","                                  class_weight=self._hyper_params.get('class_weights'),\n","                                  callbacks=callbacks)\n","\n","        return mm.ModelHistory(self._hyper_params, history.history)\n","\n","    def predict(self, test_ds):\n","        scores = self._model.predict(test_ds)\n","        return np.argmax(scores, axis=-1), scores\n","\n","#print(BertModel()._model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNRUKXBJyXHT"},"source":["##### Hyper parameter space search\n","To check how our model works on different hyperparameters do a grid search using matmih.hyperparameters.HyperParamsLookup class.\n","\n","After each model tried the tensorflow session is cleared. For the best model the weights are saved on disk\n","\n","There are a total of 32 combinations that will be tried on the original dataset as well as the one augmented with the image generator."]},{"cell_type":"code","metadata":{"id":"5I5Iizu6yXHT"},"source":["import official.nlp.optimization\n","\n","steps_per_epoch = (int)(len(trainDFsplit) / BATCH_SIZE)\n","num_train_steps = steps_per_epoch * TRAIN_EPOCHS\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","def create_optimizer():\n","    return official.nlp.optimization.create_optimizer(init_lr=3e-5,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","# Search the hyperparameter space for the best model\n","nlpHyperLookup = mm.HyperParamsLookup(lambda hyper_params: BertModel(**hyper_params),\n","                                      lambda hist: np.max(hist.history('accuracy', mm.DataType.VALIDATION)))\n","nlpHyperLookup.grid_search((trainDF, valDF),  True,\n","                           trainEpochs=[TRAIN_EPOCHS],\n","                           dropoutRate=[0.5],\n","                           class_weights=[get_class_weights(trainDFsplit)],\n","                           pooled_size=[1, 'all'],\n","                           max_words=[64, 128],\n","                           delta=[0, 16],\n","                           optimizer=[create_optimizer] )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JneDhC5yXHT"},"source":["##### Hyper parameter search model results\n","Plot the results of the hyperparameter model search.\n","\n","The plots will contain the same color for the same model. The train metrics are ploted using dotted lines."]},{"cell_type":"code","metadata":{"id":"l9GNWZREyXHT"},"source":["ev = mm.ModelEvaluation(nlpHyperLookup.history)\n","ev.plot_history('Training history', ['accuracy', 'loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEqFuH_UyXHT"},"source":["#### Bert best model retrained\n","Reload the model using the saved weights in the hyper parameter search"]},{"cell_type":"code","metadata":{"id":"HFiOj8KWyXHT"},"source":["# Open tensor board\n","#result = mm.TensorBoard.open()\n","\n","# Load the model again using the epoch with the highest validation accuracy\n","best_history = nlpHyperLookup.best_history\n","nlp_hyper_params = best_history.model_params.copy()\n","nlp_hyper_params['trainEpochs'] = np.argmax(best_history.history('accuracy', mm.DataType.VALIDATION)) + 3\n","\n","print(\"Bert Model using {}\\nvalidation accuracy={}\".format(nlp_hyper_params,\n","                                                               best_history.history('accuracy', mm.DataType.VALIDATION)))\n","\n","best_model_train = BertModel(**nlp_hyper_params)\n","best_model_train.load_weights(nlpHyperLookup.best_checkpoint)\n","\n","#best_model_train = tf.keras.models.load_model(nlpHyperLookup.best_checkpoint,\n","#                                              custom_objects={'KerasLayer': hub.KerasLayer,\n","#                                                              'AdamWeightDecay' : official.nlp.optimization.AdamWeightDecay})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"14nPv3bW12AP"},"source":["def custom_prediction(model, df, df_split):\n","    _, scores = model.predict(TestDataset(df_split))\n","\n","    class_scores = {}\n","    for i, (sid, text) in df_split[['sid', 'text']].iterrows():\n","        lst = class_scores.get(sid, [])\n","        lst.append(scores[i] * len(text) / MAX_LENGTH)\n","        class_scores[sid] = lst\n","\n","    class_targets = []\n","    class_scores_all = []\n","    for sid in df['sid']:\n","        all_scores = np.sum(np.array(class_scores[sid]), axis=0)\n","        all_scores = tf.nn.softmax(all_scores).numpy()\n","        class_targets.append(np.argmax(all_scores))\n","        class_scores_all.append(all_scores)\n","    \n","    return np.array(class_targets), np.array(class_scores_all)\n","\n","def custom_evaluate(model, df, df_split):\n","    mm.Model.accuracy(custom_prediction(model, df, df_split), df['target'].values)\n","\n","def show_predictions(model, df, df_split, title):\n","    target, scores = custom_prediction(model, df, df_split)\n","    print(f\"{title} accuracy: {mm.Model.accuracy(df['target'].values, target)}\")\n","    return target, scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eJCpNyh0j2U"},"source":["valDFsplit = split_train(valDF, nlp_hyper_params['max_words'], nlp_hyper_params['delta'])\n","\n","val_pred, val_scores = show_predictions(best_model_train, valDF, valDFsplit, 'VALIDATION')\n","\n","pb = mm.PlotBuilder().create_confusion_matrix(\n","    valDF['target'], val_pred, range(0, NO_CLASSES))\n","\n","pb.create_roc_curve_multiclass(\n","    valDF['target'], val_scores, range(0, NO_CLASSES), [True] * NO_CLASSES).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"teKxeXppyXHT"},"source":["### Additional training on the train and validation set"]},{"cell_type":"code","metadata":{"id":"OjtcGRBH7_aG"},"source":["import time\n","from sklearn.model_selection import train_test_split\n","\n","valDFtrain, remainingDFVal = train_test_split(valDF, test_size=0.2,\n","                                              shuffle=True, random_state=int(round(time.time())),\n","                                              stratify=valDF['target'])\n","\n","allDFdata = pd.concat([trainDF, valDFtrain])\n","remainingVal_split = split_train(valDF, nlp_hyper_params['max_words'], nlp_hyper_params['delta'])\n","\n","print(f\"All train data: {len(allDFdata)}\")\n","print(f\"Validation remaining: {len(remainingDFVal)} split: {len(remainingVal_split)}\")\n","\n","_, _ = show_predictions(best_model_train, remainingDFVal, remainingVal_split, \"REMAINING VALIDATION\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJ2VoqYOBetF"},"source":["retrain the model on all data"]},{"cell_type":"code","metadata":{"id":"6CoKjKYeyXHU"},"source":["import gc\n","tf.keras.backend.clear_session()\n","try:\n","    del best_model_train\n","except:\n","    pass\n","gc.collect()\n","\n","nlp_hyper_params['trainEpochs'] = 2*TRAIN_EPOCHS\n","nlp_hyper_params['optimizer'] = create_optimizer\n","\n","best_model_retrained = BertModel(**nlp_hyper_params)\n","\n","history_retrain = best_model_retrained.train((allDFdata, remainingDFVal))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPSr9UYbL-4E"},"source":["# Load the model again using the epoch with the highest validation accuracy\n","best_history = history_retrain\n","best_hyper_params = best_history.model_params.copy()\n","best_hyper_params['trainEpochs'] = np.argmax(best_history.history('accuracy', mm.DataType.VALIDATION))\n","\n","print(\"Bert Model using {} \\nvalidation accuracy={}\".format(best_hyper_params,\n","                                                            best_history.history('accuracy', mm.DataType.VALIDATION)))\n","\n","best_model = BertModel(**best_hyper_params)\n","best_model.load_weights(best_model_retrained.best_weights_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCX-vzhLyXHT"},"source":["#### NLP Model metrics\n","Plot the confusion matrix and Receiver Operating Caracteristic curve for the best model.\n","\n","The ROC curve will be plotted for each class as well as an micro averaged dotted one for all classes.\n","\n","In a binary classification an AUC of 0.5 (blue diagonal line) means that the model has no discriminative capacity to differenciate between the 2 classes.\n","\n","When AUC is 0.7, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n","Usually an AUC of 0.8 is considered good."]},{"cell_type":"code","metadata":{"id":"PzzlialnyXHT"},"source":["# Plot the confusion matrix and roc curve for the remainer of the validation set\n","pred_rem_targets, pred_rem_scores_ = show_predictions(best_model, remainingDFVal, remainingVal_split, \"REMAINING VALIDATION\")\n","\n","pb = mm.PlotBuilder().create_confusion_matrix(\n","    remainingDFVal['target'], pred_rem_targets, range(0, NO_CLASSES))\n","\n","pb.create_roc_curve_multiclass(\n","    remainingDFVal['target'], pred_rem_scores_, range(0, NO_CLASSES), [True] * NO_CLASSES).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6Qxd-nPyXHU"},"source":["#### Model selection\n","Run the 2 models on the *test set* and check accuracy and metrics\n","\n","Plot the test class distribution.\n","\n","Plot the confusion matrix of the CNN model vs SVM model. Notice what classes both models predicted correctly"]},{"cell_type":"code","metadata":{"id":"FwBNPIWqyXHU"},"source":["testDFsplit = split_train(testDF, nlp_hyper_params['max_words'], nlp_hyper_params['delta'])\n","\n","test_pred, test_scores = custom_prediction(best_model, testDF, testDFsplit)\n","\n","pb = mm.PlotBuilder().create_histograms([(test_pred, range(0, NO_CLASSES))], ['Predicted'])\n","pb.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6RIj1YJyXHU"},"source":["### Save the result"]},{"cell_type":"code","metadata":{"id":"8n0frI_tyXHU"},"source":["os.makedirs(OUTPUT_DIR, exist_ok=True)\n","output_path = f'{OUTPUT_DIR}/Mihai_Matei_submission_{datetime.now().strftime(\"%d_%m_%Y_%H_%M\")}.txt'\n","with open(output_path, 'w') as f:\n","    f.write('id,label\\n')\n","    for i in range(len(test_pred)):\n","        f.write(\"{},{}\\n\".format(testDF['sid'].iloc[i], test_pred[i]+1))"],"execution_count":null,"outputs":[]}]}