{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Hallucination Classification\n",
    "\n",
    "Mihai Matei [Data Science]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "DATA_DIR='data_cv'\n",
    "OUTPUT_DIR='output_cv'\n",
    "\n",
    "def mount_gdrive():\n",
    "    # Import the library and kaggle config files from gdrive\n",
    "    GDRIVE_PATH='/content/gdrive/RESEARCH'\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import drive\n",
    "        import shutil\n",
    "        drive.mount('/content/gdrive')\n",
    "        sys.path.append(GDRIVE_PATH)\n",
    "        os.makedirs('/root/.kaggle/', exist_ok=True)\n",
    "        shutil.copyfile(GDRIVE_PATH + 'kaggle.json', '/root/.kaggle/kaggle.json')\n",
    "        !chmod 600 '/root/.kaggle/kaggle.json'\n",
    "\n",
    "def install_modules():\n",
    "    !pip install --quiet kaggle\n",
    "    !pip install --quiet pandas\n",
    "    !pip install --quiet scikit-learn\n",
    "    !pip install --quiet scikit-image\n",
    "    !pip install --quiet scipy\n",
    "    !pip install --quiet statsmodels\n",
    "    !pip install --upgrade --quiet tensorflow\n",
    "    !pip install --upgrade --quiet tensorflow-probability\n",
    "    !pip install --quiet randomcolor\n",
    "    !pip install --quiet matplotlib\n",
    "    !pip install --quiet Pillow\n",
    "    !pip install --quiet arviz\n",
    "    !pip install --quiet seaborn\n",
    "    !pip install --quiet prettytable\n",
    "\n",
    "mount_gdrive()\n",
    "#install_modules()\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.python.platform.build_info.build_info)\n",
    "\n",
    "TRAIN_EPOCHS=50\n",
    "VALIDATION_TRAIN_EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/glypher/matmih.git\n",
    "!git -C matmih pull\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "import matmih as mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the database\n",
    "Create a kaggle account and an API token \n",
    "https://github.com/Kaggle/kaggle-api/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the kaggle competition database\n",
    "if not os.path.isfile(f'./{DATA_DIR}/train.txt'):\n",
    "    import kaggle, zipfile\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.competition_download_files('dl-2020-unibuc-cv', path=f'./{DATA_DIR}')\n",
    "    with zipfile.ZipFile(f'./{DATA_DIR}/dl-2020-unibuc-cv.zip') as z:\n",
    "        z.extractall(f'./{DATA_DIR}')\n",
    "    os.remove(f'./{DATA_DIR}/dl-2020-unibuc-cv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallucination data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def show_random_classes(df):\n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    random.seed(datetime.now())\n",
    "    pb = mm.PlotBuilder()\n",
    "    for cl in df['target'].cat.categories:\n",
    "        class_df = df[df['target'] == cl]\n",
    "        images = []\n",
    "        titles = []\n",
    "        for i in range(6):\n",
    "            idx = random.randrange(len(class_df))\n",
    "            titles.append('Class: {} If:{}'.format(cl, idx))\n",
    "            images.append(class_df['path'].iloc[idx])\n",
    "        pb.create_images(images, titles)\n",
    "    pb.show()\n",
    "\n",
    "def load_data(data_type):\n",
    "    df = pd.read_csv(f\"./{DATA_DIR}/{data_type}.txt\", delimiter=',', names=['path', 'target'])\n",
    "    df.dataframeName = data_type\n",
    "    \n",
    "    df['target'] = df['target'].astype('category')\n",
    "    df['path'] = df['path'].apply(lambda row: os.path.join(DATA_DIR, data_type, row))\n",
    "    df['features'] = df['path'].apply(lambda row: mm.Image.load(row, size=(32,32)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "trainDF = load_data('train')\n",
    "valDF = load_data('validation')\n",
    "testDF = load_data('test')\n",
    "mm.PlotBuilder().create_histograms([trainDF['target'], valDF['target']],\n",
    "                                   ['Class histogram TRAIN', 'Class histogram VALIDATION']).show()\n",
    "show_random_classes(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = mm.DataModel(None, None)\n",
    "data_model.set_train(np.stack( trainDF['features'].values ), trainDF['target'].values)\n",
    "data_model.set_validation(np.stack( valDF['features'].values ), valDF['target'].values)\n",
    "data_model.set_test(np.stack( testDF['features'].values ), testDF['target'].values)\n",
    "\n",
    "# Normalize the features before proceeding\n",
    "# This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8\n",
    "#  https://arxiv.org/abs/1801.04381\n",
    "data_model = data_model.normalize_MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "Use transfer learning to create a CNN Model starting from MobileNetV2.\n",
    "\n",
    "All the models extend the matmih.model.Model class.\n",
    "\n",
    "Added a multitude of hyperparameters to be tried:\n",
    "* denseSize - first dense layer size\n",
    "* denseL2 - first dense layer L2 regularization\n",
    "* dropoutRate - dropout layer percentage\n",
    "* trainEpochs - number of epochs to train\n",
    "* class_weights - if the target class distribution should be taken into account when computing the loss\n",
    "* optimizer - the Keras optimizer to use (will also include parameters as learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "class CNNModel(mm.TensorModel):\n",
    "    # Create the base model from the pre-trained model MobileNet V2\n",
    "    _base_model = tf.keras.applications.MobileNetV2(input_shape=(32, 32, 3),\n",
    "                                                    weights=None,\n",
    "                                                    include_top=False,\n",
    "                                                    pooling='avg',\n",
    "                                                   classes=8)\n",
    "    _base_model.trainable = True\n",
    "    # Uncomment the below line to get a summary of the layers involved in MobilNetV2\n",
    "    _base_model.summary()\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    def __init__(self, **hyper_params):\n",
    "        self._hyper_params = hyper_params.copy()\n",
    "        model = tf.keras.Sequential([\n",
    "                        CNNModel._base_model,\n",
    "                        tf.keras.layers.Dense(hyper_params.get('denseSize', 128),\n",
    "                                              kernel_initializer='glorot_uniform',\n",
    "                                              kernel_regularizer=tf.keras.regularizers.l2(hyper_params.get('denseL2', 0.01)),\n",
    "                                              bias_regularizer=tf.keras.regularizers.l2(hyper_params.get('denseL2', 0.01))),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'),\n",
    "                        tf.keras.layers.Dropout(hyper_params.get('dropoutRate', 0.5)),\n",
    "                        tf.keras.layers.Dense(hyper_params.get('noClasses', 8)),\n",
    "                        tf.keras.layers.Activation('softmax')\n",
    "                                              ])\n",
    "        super(CNNModel, self).__init__(model)\n",
    "\n",
    "        self._train_epochs = hyper_params.get('trainEpochs', 20)\n",
    "        self._optimizer = hyper_params.get('optimizer', tf.keras.optimizers.RMSprop())\n",
    "        self._class_weights = hyper_params.get('class_weights', False)\n",
    "\n",
    "        # compile the model and initialize the weights\n",
    "        self._model.compile(\n",
    "             optimizer=self._optimizer,\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(5)])\n",
    "\n",
    "    # Convert the features/target np data to a tensorflow dataset\n",
    "    @staticmethod\n",
    "    def np_to_tf(features, target=None, batch_size=BATCH_SIZE):\n",
    "        if target is None:\n",
    "            ds = tf.data.Dataset.from_tensor_slices( (tf.cast(features, tf.float32)) )\n",
    "        else:\n",
    "            ds = tf.data.Dataset.from_tensor_slices( (tf.cast(features, tf.float32),\n",
    "                                                      tf.cast(target, tf.int32)) )\n",
    "\n",
    "        return ds if batch_size is None else ds.batch(batch_size)\n",
    "\n",
    "    def train(self, data_model, logTensorBoard=False):\n",
    "        # save the weights of the best epoch to disk\n",
    "        callbacks = [tf.keras.callbacks.ModelCheckpoint(self.best_weights_path, monitor='val_accuracy',\n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=True, mode='auto')]\n",
    "        if logTensorBoard:\n",
    "            callbacks += [tf.keras.callbacks.TensorBoard(mm.TensorBoard.get_log_dir(), histogram_freq=1)]\n",
    "\n",
    "        train_ds = CNNModel.np_to_tf(data_model.train_features, data_model.train_target)\n",
    "        validation_ds = CNNModel.np_to_tf(data_model.validation_features, data_model.validation_target)\n",
    "        \n",
    "        # compute the class weights to give to the model train target classes\n",
    "        class_weights = None\n",
    "        if self._class_weights:\n",
    "            class_values = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "                                                                           data_model.classes, data_model.train_target)\n",
    "            class_values = class_values / np.min(class_values)\n",
    "            class_weights = {c:class_values[c] for c in range(len(class_values))}\n",
    "\n",
    "        history = self._model.fit(train_ds, validation_data=validation_ds,\n",
    "                                  class_weight=class_weights,\n",
    "                                  epochs=self._train_epochs, callbacks=callbacks)\n",
    "\n",
    "        return mm.ModelHistory(self._hyper_params, history.history)\n",
    "\n",
    "    def predict(self, features):\n",
    "        features_ds = tf.cast(features, tf.float32)\n",
    "        scores = self._model.predict(features_ds)\n",
    "        return np.argmax(scores, axis=-1), scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper parameter space search\n",
    "To check how our model works on different hyperparameters do a grid search using matmih.hyperparameters.HyperParamsLookup class.\n",
    "\n",
    "After each model tried the tensorflow session is cleared. For the best model the weights are saved on disk\n",
    "\n",
    "There are a total of 32 combinations that will be tried on the original dataset as well as the one augmented with the image generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the hyperparameter space for the best model\n",
    "cnnHyperLookup = mm.HyperParamsLookup(CNNModel(), lambda hist: np.max(hist.history('accuracy', mm.DataType.VALIDATION)))\n",
    "cnnHyperLookup.grid_search(data_model, True,\n",
    "                           noClasses=[8],\n",
    "                           trainEpochs=[TRAIN_EPOCHS],\n",
    "                           denseSize=[128],\n",
    "                           denseL2=[0.01],\n",
    "                           dropoutRate=[0.5],\n",
    "                           class_weights=[False],\n",
    "                           optimizer=[tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "                                      #tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "                                      #tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.2, nesterov=True),\n",
    "                                      tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=False)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper parameter search model results\n",
    "Plot the results of the hyperparameter model search.\n",
    "\n",
    "The plots will contain the same color for the same model. The train metrics are ploted using dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = mm.ModelEvaluation(cnnHyperLookup.history)\n",
    "ev.plot_history('Training history', ['accuracy', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN best model retrained\n",
    "Reload the model using the saved weights in the hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tensor board\n",
    "#result = mm.TensorBoard.open()\n",
    "\n",
    "# Load the model again using the epoch with the highest validation accuracy\n",
    "best_history = cnnHyperLookup.best_history\n",
    "cnn_hyper_params = best_history.model_params.copy()\n",
    "cnn_hyper_params['trainEpochs'] = np.argmax(best_history.history('accuracy', mm.DataType.VALIDATION)) + 1\n",
    "\n",
    "print(\"CNN Model using {} lr={}\\nvalidation accuracy={}\".format(cnn_hyper_params,\n",
    "                                                               tf.keras.backend.eval(cnn_hyper_params['optimizer'].lr),\n",
    "                                                               best_history.history('accuracy', mm.DataType.VALIDATION)))\n",
    "cnn_model = CNNModel(**cnn_hyper_params)\n",
    "# Load the best checkpoint found the in training\n",
    "cnn_model.load_weights(cnnHyperLookup.best_checkpoint)\n",
    "\n",
    "# Save the model to the disk\n",
    "cnn_model.save_model(name='best_model_{}'.format(datetime.now().strftime(\"%d_%m_%Y_%H_%M\")))\n",
    "# Optional load model\n",
    "#cnn_model = mm.TensorModel.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model metrics\n",
    "Plot the confusion matrix and Receiver Operating Caracteristic curve for the best model.\n",
    "\n",
    "The ROC curve will be plotted for each class as well as an micro averaged dotted one for all classes.\n",
    "\n",
    "In a binary classification an AUC of 0.5 (blue diagonal line) means that the model has no discriminative capacity to differenciate between the 2 classes.\n",
    "\n",
    "When AUC is 0.7, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "Usually an AUC of 0.8 is considered good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and roc curve for the validation set\n",
    "pred_targets, pred_scores = cnn_model.predict(data_model.validation_features)\n",
    "\n",
    "pb = mm.PlotBuilder().create_confusion_matrix(\n",
    "    data_model.validation_target, pred_targets, range(0, 8))\n",
    "\n",
    "pb.create_roc_curve_multiclass(\n",
    "    data_model.validation_target, pred_scores, range(0, 8), [True] * 8).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional training on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CNNModel.np_to_tf(data_model.validation_features, data_model.validation_target)\n",
    "history = cnn_model._model.fit(train_ds, epochs=VALIDATION_TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "Run the 2 models on the *test set* and check accuracy and metrics\n",
    "\n",
    "Plot the test class distribution.\n",
    "\n",
    "Plot the confusion matrix of the CNN model vs SVM model. Notice what classes both models predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_target, cnn_test_scores = cnn_model.predict(data_model.test_features)\n",
    "\n",
    "pb = mm.PlotBuilder().create_histograms([(cnn_test_target, range(0,8))], ['Predicted'])\n",
    "pb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = f'{OUTPUT_DIR}/Mihai_Matei_submission_{datetime.now().strftime(\"%d_%m_%Y_%H_%M\")}.txt'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i in range(len(cnn_test_target)):\n",
    "        f.write(\"{},{}\\n\".format(os.path.basename(testDF['path'].iloc[i]), cnn_test_target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
