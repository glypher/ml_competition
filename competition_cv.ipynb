{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EcuyW3uSnUM"
   },
   "source": [
    "#### Deep Hallucination Classification\n",
    "\n",
    "Mihai Matei [Data Science]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjlIA_UPSnUM",
    "outputId": "5fa08849-c256-4b3c-fb40-2c930a59e0b5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "DATA_DIR='data_cv'\n",
    "OUTPUT_DIR='output_cv'\n",
    "\n",
    "MODEL_CHECKPOINT='./model_checkpoint'\n",
    "BEST_MODEL_DIR='best_model_cv'\n",
    "\n",
    "def mount_gdrive():\n",
    "    global DATA_DIR\n",
    "    global OUTPUT_DIR\n",
    "    # Import the library and kaggle config files from gdrive\n",
    "    GDRIVE_PATH='/content/drive/MyDrive/RESEARCH/'\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import drive\n",
    "        import shutil\n",
    "        drive.mount('/content/drive/')\n",
    "        sys.path.append(GDRIVE_PATH)\n",
    "        os.makedirs('/root/.kaggle/', exist_ok=True)\n",
    "        shutil.copyfile(GDRIVE_PATH + 'kaggle.json', '/root/.kaggle/kaggle.json')\n",
    "        !chmod 600 '/root/.kaggle/kaggle.json'\n",
    "        DATA_DIR = os.path.join(GDRIVE_PATH, DATA_DIR)\n",
    "        OUTPUT_DIR = os.path.join(GDRIVE_PATH, OUTPUT_DIR)\n",
    "\n",
    "def install_modules():\n",
    "    !pip install --quiet kaggle\n",
    "    !pip install --quiet pandas\n",
    "    !pip install --quiet scikit-learn\n",
    "    !pip install --quiet scikit-image\n",
    "    !pip install --quiet scipy\n",
    "    !pip install --quiet statsmodels\n",
    "    !pip install --upgrade --quiet tensorflow\n",
    "    !pip install --upgrade --quiet tensorflow-probability\n",
    "    !pip install --quiet randomcolor\n",
    "    !pip install --quiet matplotlib\n",
    "    !pip install --quiet Pillow\n",
    "    !pip install --quiet arviz\n",
    "    !pip install --quiet seaborn\n",
    "    !pip install --quiet prettytable\n",
    "    !pip install --quiet tf-models-official\n",
    "\n",
    "mount_gdrive()\n",
    "#install_modules()\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_CHECKPOINT, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_CHECKPOINT, BEST_MODEL_DIR), exist_ok=True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.python.platform.build_info.build_info)\n",
    "\n",
    "\n",
    "IMAGE_SIZE=(32, 32)\n",
    "#LARGE_SIZE=(96, 96)\n",
    "LARGE_SIZE=(32, 32)\n",
    "#LARGE_SIZE=(224, 224)\n",
    "\n",
    "AUGMENT=False\n",
    "\n",
    "NO_CLASSES=8\n",
    "BATCH_SIZE=128\n",
    "TRAIN_EPOCHS=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106987,
     "status": "ok",
     "timestamp": 1606510823483,
     "user": {
      "displayName": "Mihai Matei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiniPXEs1itGGnxwLSoz5k-dDrj2tfHpef9Lmit=s64",
      "userId": "01318548799329859257"
     },
     "user_tz": -120
    },
    "id": "0bCIGYaASnUN",
    "outputId": "b8fcdaf7-017a-41a0-caf6-50e068e2cef7"
   },
   "outputs": [],
   "source": [
    "!git clone 'https://glypher:886cd6845d8a78081ae9cd4b6c259722f3b7ca3e@github.com/glypher/matmih.git'\n",
    "!git -C matmih pull\n",
    "\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "import matmih as mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPu5lwoaSnUN"
   },
   "source": [
    "#### Download the database\n",
    "Create a kaggle account and an API token \n",
    "https://github.com/Kaggle/kaggle-api/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1337,
     "status": "ok",
     "timestamp": 1606510824824,
     "user": {
      "displayName": "Mihai Matei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiniPXEs1itGGnxwLSoz5k-dDrj2tfHpef9Lmit=s64",
      "userId": "01318548799329859257"
     },
     "user_tz": -120
    },
    "id": "5p5ioqctSnUN"
   },
   "outputs": [],
   "source": [
    "# Download the kaggle competition database\n",
    "if not os.path.isfile(f'{DATA_DIR}/train.txt'):\n",
    "    import kaggle, zipfile\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.competition_download_files('dl-2020-unibuc-cv', path=f'{DATA_DIR}')\n",
    "    with zipfile.ZipFile(f'{DATA_DIR}/dl-2020-unibuc-cv.zip') as z:\n",
    "        z.extractall(f'{DATA_DIR}')\n",
    "    os.remove(f'{DATA_DIR}/dl-2020-unibuc-cv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ZtlZvZSnUN"
   },
   "source": [
    "#### Hallucination data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 37094,
     "status": "ok",
     "timestamp": 1606510464439,
     "user": {
      "displayName": "Mihai Matei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiniPXEs1itGGnxwLSoz5k-dDrj2tfHpef9Lmit=s64",
      "userId": "01318548799329859257"
     },
     "user_tz": -120
    },
    "id": "xBwsw-wbSnUN",
    "outputId": "da2dda07-c955-455c-d9a7-0f3c2d7e2a0c"
   },
   "outputs": [],
   "source": [
    "def show_random_classes(df):\n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    random.seed(datetime.now())\n",
    "    pb = mm.PlotBuilder()\n",
    "    for cl in df['target'].cat.categories:\n",
    "        class_df = df[df['target'] == cl]\n",
    "        images = []\n",
    "        titles = []\n",
    "        for i in range(6):\n",
    "            idx = random.randrange(len(class_df))\n",
    "            titles.append('Class: {} If:{}'.format(cl, idx))\n",
    "            images.append(class_df['features'].iloc[idx])\n",
    "        pb.create_subplots(NO_CLASSES, 6, (10,10), dpi=80).create_images(images, titles)\n",
    "    pb.show()\n",
    "\n",
    "def load_data(data_type):\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/{data_type}.txt\", delimiter=',', names=['path', 'target'])\n",
    "    df.dataframeName = data_type\n",
    "    \n",
    "    df['target'] = df['target'].astype('category')\n",
    "    df['path'] = df['path'].apply(lambda row: os.path.join(DATA_DIR, data_type, row))\n",
    "    df['features'] = df['path'].apply(lambda row: mm.Image.load(row, size=IMAGE_SIZE))\n",
    "\n",
    "    return df\n",
    "\n",
    "trainDF = load_data('train')\n",
    "valDF = load_data('validation')\n",
    "testDF = load_data('test')\n",
    "mm.PlotBuilder().create_subplots(1,2, (13,6)).create_histograms(\n",
    "    [trainDF['target'], valDF['target']],\n",
    "    ['Class histogram TRAIN', 'Class histogram VALIDATION']).show()\n",
    "show_random_classes(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can augment our training data by generating modifed images and adding them to our training set\n",
    "# I do this so to try to balance the current class distributions, by generating more images for unbalanced data\n",
    "def augment_training_data(df, iterations):\n",
    "    features = np.stack(df['features'].values)\n",
    "    targets = np.stack(df['target'].values)\n",
    "    image_generator = mm.ImageGenerator(features, targets, balanced=True, power=8,\n",
    "                                        rotation_range=20,\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "    new_images = []\n",
    "    new_targets = []\n",
    "    # augment the train dataset with the image generator\n",
    "    for image_features, targets in image_generator.generate(iterations, batch_size=32):\n",
    "        for i in range(32):\n",
    "            new_images.append(image_features[i])\n",
    "            new_targets.append(targets[i])\n",
    "\n",
    "    new_df = pd.DataFrame({'features' : new_images, 'target' : new_targets,\n",
    "                           'path': ['generated'] * len(new_images)})\n",
    "    new_df['target'] = new_df['target'].astype('category')\n",
    "\n",
    "    return new_df\n",
    "\n",
    "if AUGMENT:\n",
    "    trainDF = pd.concat([trainDF, augment_training_data(trainDF, 140)])\n",
    "    trainDF['target'] = trainDF['target'].astype('category')\n",
    "\n",
    "    mm.PlotBuilder().create_subplots(1,1, (13,6)).create_histograms(\n",
    "    [trainDF['target']], ['Class histogram after augment TRAIN']).show()\n",
    "\n",
    "    show_random_classes(trainDF[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "NOISE_STD=0.02\n",
    "\n",
    "def _load_image(features, img_size=IMAGE_SIZE, augment_data=False):\n",
    "    img = tf.cast(features, tf.int32)\n",
    "\n",
    "    if img_size != features.shape[0:2]:\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        \n",
    "    if augment_data:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_saturation(img, 2, 5)\n",
    "        img = tf.image.random_contrast(img, 0.9, 1)\n",
    "\n",
    "    #img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img / 255.\n",
    "\n",
    "    if augment_data:\n",
    "        img = img + tf.random.normal(img_size + (3,), mean=0.0, stddev=NOISE_STD)\n",
    "        img = tf.clip_by_value(img, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "     # does RGB -> [0,1] normalization\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoderDataset(tf.data.Dataset):\n",
    "    def __new__(cls, df, img_size=IMAGE_SIZE):\n",
    "        def _load_data(path):\n",
    "            inputs = _load_image(path, img_size)\n",
    "\n",
    "            return inputs, tf.identity(inputs['input'])\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df['path'].values, tf.string)))\n",
    "                                \n",
    "        return dataset.shuffle(1000).map(\n",
    "            lambda x : _load_data(x),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(\n",
    "            BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "class TrainDataset(tf.data.Dataset):\n",
    "    def __new__(cls, df, img_size=IMAGE_SIZE[0:2], augment_data=False):\n",
    "        def _generator_data():\n",
    "            for _, row in df.iterrows():\n",
    "                features = _load_image(row['features'], img_size, augment_data)\n",
    "                \n",
    "                yield (features, tf.cast(row['target'], tf.int32))\n",
    "                \n",
    "        IMG_SHAPE = img_size + (3,)\n",
    "        return tf.data.Dataset.from_generator(_generator_data, (tf.float32, tf.int32),\n",
    "                                              (tf.TensorShape([*IMG_SHAPE]), tf.TensorShape([]))).shuffle(\n",
    "            3000).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "class TestDataset(tf.data.Dataset):\n",
    "    def __new__(cls, df, img_size=IMAGE_SIZE):\n",
    "        def _generator_data():\n",
    "            for _, row in df.iterrows():\n",
    "                features = _load_image(row['features'], img_size)\n",
    "                \n",
    "                yield (features)\n",
    "                \n",
    "        IMG_SHAPE = img_size + (3,)\n",
    "        return tf.data.Dataset.from_generator(_generator_data, (tf.float32),\n",
    "                                              (tf.TensorShape([*IMG_SHAPE]))).batch(\n",
    "            BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    \n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    import time\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            pass\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "\n",
    "#ds = TrainDataset(trainDF)\n",
    "#benchmark(ds)\n",
    "for r in TrainDataset(trainDF[:1], img_size=IMAGE_SIZE, augment_data=True).take(1):\n",
    "    img, _ = r\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow((img[0]*255).numpy().astype(np.int32))\n",
    "    plt.show()\n",
    "    \n",
    "for img in TestDataset(trainDF[:1], img_size=IMAGE_SIZE).take(1):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow((img[0]*255).numpy().astype(np.int32))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3v7apbCSnUN"
   },
   "source": [
    "### CNN Model\n",
    "Use transfer learning to create a CNN Model starting from MobileNetV2.\n",
    "\n",
    "All the models extend the matmih.model.Model class.\n",
    "\n",
    "Added a multitude of hyperparameters to be tried:\n",
    "* denseSize - first dense layer size\n",
    "* denseL2 - first dense layer L2 regularization\n",
    "* dropoutRate - dropout layer percentage\n",
    "* trainEpochs - number of epochs to train\n",
    "* class_weights - if the target class distribution should be taken into account when computing the loss\n",
    "* optimizer - the Keras optimizer to use (will also include parameters as learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54211,
     "status": "ok",
     "timestamp": 1606510523021,
     "user": {
      "displayName": "Mihai Matei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiniPXEs1itGGnxwLSoz5k-dDrj2tfHpef9Lmit=s64",
      "userId": "01318548799329859257"
     },
     "user_tz": -120
    },
    "id": "XG8VaxS_SnUN"
   },
   "outputs": [],
   "source": [
    "class CNNModel(mm.TensorModel):\n",
    "    @staticmethod\n",
    "    def build_model(**kwargs):\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        def _conv_layer(inputs, filter_no, filter_size=(3, 3), conv_rnn=False):\n",
    "            layer = layers.Conv2D(filter_no, filter_size,\n",
    "                                  kernel_initializer='he_uniform', padding='same')(inputs)\n",
    "            layer = layers.BatchNormalization()(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            if conv_rnn:\n",
    "                skip = layer\n",
    "            layer = layers.Conv2D(filter_no, filter_size,\n",
    "                                  kernel_initializer='he_uniform', padding='same')(layer)\n",
    "            layer = layers.BatchNormalization()(layer)\n",
    "            layer = layers.Activation('relu')(layer)\n",
    "            if conv_rnn:\n",
    "                layer = layer = layers.Concatenate()([layer, skip])\n",
    "            layer = layers.MaxPooling2D((2, 2))(layer)\n",
    "            layer = layers.Dropout(kwargs.get('conv_dropout'))(layer)\n",
    "\n",
    "            return layer\n",
    "\n",
    "\n",
    "        inputs = tf.keras.Input(shape=IMAGE_SIZE + (3,), name=\"input\")\n",
    "\n",
    "        filter_sizes = kwargs.get('filter_size')\n",
    "        layer = inputs\n",
    "        for filter_size in filter_sizes:\n",
    "            layer = _conv_layer(layer, filter_size)\n",
    "\n",
    "        layer = layers.GlobalAveragePooling2D()(layer)\n",
    "        \n",
    "        layer = layers.Dense(128, kernel_initializer='he_uniform',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(0.001))(layer)\n",
    "        layer = layers.BatchNormalization()(layer)\n",
    "        layer = layers.Activation('relu')(layer)\n",
    "        layer = layers.Dropout(kwargs.get('dense_dropout'))(layer)\n",
    "        layer = layers.Dense(NO_CLASSES, activation='softmax', name=\"CLASS_OUTPUT\")(layer)\n",
    "        classifier = layer\n",
    "\n",
    "        return inputs, classifier\n",
    "\n",
    "    def __init__(self, **hyper_params):\n",
    "        self._hyper_params = hyper_params.copy()\n",
    "\n",
    "        inputs, classifier = CNNModel.build_model(**self._hyper_params)\n",
    "        #self._cnn_part_encoder = cnn_part_encoder\n",
    "        #self._layer_transfer = layer_transfer\n",
    "\n",
    "        #self._model_autoencoder = tf.keras.Model(inputs=inputs, outputs=decoder)\n",
    "        #self._model_transfer_learning = tf.keras.Model(inputs=inputs_large, outputs=tf_hub_classifier)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=classifier)\n",
    "\n",
    "        super(CNNModel, self).__init__(model,\n",
    "                                       checkpoint=True)\n",
    "\n",
    "        self._train_epochs = hyper_params.get('trainEpochs')\n",
    "        self._optimizer = hyper_params.get('optimizer')\n",
    "        \n",
    "        losses = {\"DECODER_OUTPUT\": \"mean_squared_error\",\n",
    "                  \"CLASS_OUTPUT\": \"sparse_categorical_crossentropy\"\n",
    "                 }\n",
    "        loss_weights = {\"DECODER_OUTPUT\": 1.0,\n",
    "                        \"CLASS_OUTPUT\": 1.0\n",
    "                       }\n",
    "        \n",
    "        metrics = {\"DECODER_OUTPUT\": \"mean_absolute_error\",\n",
    "                   \"CLASS_OUTPUT\": \"accuracy\"\n",
    "                  }\n",
    "\n",
    "        #self._model_autoencoder.compile(\n",
    "         #    optimizer=self._optimizer,\n",
    "         #    loss=losses['DECODER_OUTPUT'],\n",
    "         #    metrics=metrics['DECODER_OUTPUT'])\n",
    "        \n",
    "        #self._model_transfer_learning.compile(\n",
    "         #    optimizer=self._optimizer,\n",
    "         #    loss=losses['CLASS_OUTPUT'],\n",
    "         #    metrics=metrics['CLASS_OUTPUT'])\n",
    "        \n",
    "        # compile the model and initialize the weights\n",
    "        self._model.compile(\n",
    "             optimizer=self._optimizer(),\n",
    "             loss=losses['CLASS_OUTPUT'],\n",
    "             metrics=metrics['CLASS_OUTPUT'])\n",
    "        \n",
    "        # load initial checkpoints if they are available\n",
    "        if self._hyper_params.get('load_initial_weights'):\n",
    "            initial_weights = 'initial_weights_' + '_'.join(map(lambda x:str(x), self._hyper_params['filter_size']))\n",
    "            initial_weights = os.path.join(MODEL_CHECKPOINT, initial_weights)\n",
    "            if not os.path.exists(initial_weights):\n",
    "                self._model.save_weights(initial_weights)\n",
    "\n",
    "            self._model.load_weights(initial_weights)\n",
    "        \n",
    "\n",
    "    def train_autoencoder(self, trainDF):\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error',\n",
    "                                                      min_delta=1e-3, patience=2)]\n",
    "        history = self._model_autoencoder.fit(\n",
    "            AutoEncoderDataset(trainDF),\n",
    "            epochs=self._train_epochs, callbacks=callbacks)\n",
    "\n",
    "        return mm.ModelHistory(self._hyper_params, history.history)\n",
    "    \n",
    "    def train_transfer_learning(self, trainDF, valDF):\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                                      min_delta=0.001, mode='max', patience=3)]\n",
    "\n",
    "        history = self._model_transfer_learning.fit(\n",
    "            TrainDataset(trainDF, img_size=LARGE_SIZE),\n",
    "            validation_data=TrainDataset(valDF, img_size=LARGE_SIZE),\n",
    "            epochs=self._train_epochs, callbacks=callbacks)\n",
    "\n",
    "        return mm.ModelHistory(self._hyper_params, history.history)\n",
    "        \n",
    "    def train(self, data : list, logTensorBoard=False):\n",
    "        train_ds = data[0]\n",
    "        val_ds = data[1]\n",
    "\n",
    "        callbacks = []\n",
    "        # save the weights of the best epoch to disk\n",
    "        callbacks += [tf.keras.callbacks.ModelCheckpoint(self.best_weights_path, monitor='val_accuracy',\n",
    "                                                         verbose=0, save_best_only=True,\n",
    "                                                         save_weights_only=True, mode='auto')]\n",
    "        if val_ds is not None:\n",
    "            callbacks += [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                           min_delta=0.001, mode='max', patience=10)]\n",
    "            callbacks += [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                                               patience=4, min_lr=0.001)]\n",
    "        \n",
    "        if logTensorBoard:\n",
    "            callbacks += [tf.keras.callbacks.TensorBoard(mm.TensorBoard.get_log_dir(), histogram_freq=1)]\n",
    "            \n",
    "        # freeze cnn part of the encoder\n",
    "        #self._cnn_part_encoder.trainable = False\n",
    "        #self._layer_transfer.trainable = False\n",
    "           \n",
    "        train_ds = TrainDataset(train_ds, img_size=IMAGE_SIZE)\n",
    "        if self._hyper_params.get('augment'):\n",
    "            train_ds_augment = TrainDataset(data[0], img_size=IMAGE_SIZE, augment_data=True)\n",
    "            train_ds = train_ds.concatenate(train_ds_augment).shuffle(5000)\n",
    "        \n",
    "        val_ds = TrainDataset(val_ds, img_size=IMAGE_SIZE) if data[1] is not None else None\n",
    " \n",
    "        history = self._model.fit(train_ds, validation_data=val_ds,\n",
    "                                  epochs=self._train_epochs,\n",
    "                                  class_weight=self._hyper_params.get('class_weights'),\n",
    "                                  callbacks=callbacks)\n",
    "                      \n",
    "        return mm.ModelHistory(self._hyper_params, history.history)\n",
    "\n",
    "    def predict(self, dataDF, with_images=False):\n",
    "        imgs = None\n",
    "\n",
    "        scores = self._model.predict(TestDataset(dataDF, img_size=IMAGE_SIZE))\n",
    "\n",
    "        return imgs, np.argmax(scores, axis=-1), scores\n",
    "    \n",
    "\n",
    "#model = CNNModel(noClasses=NO_CLASSES, trainEpochs=TRAIN_EPOCHS, optimizer=optimizer)\n",
    "#model._model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029YHZ6BSnUN"
   },
   "source": [
    "##### Hyper parameter space search\n",
    "To check how our model works on different hyperparameters do a grid search using matmih.hyperparameters.HyperParamsLookup class.\n",
    "\n",
    "After each model tried the tensorflow session is cleared. For the best model the weights are saved on disk\n",
    "\n",
    "There are a total of 32 combinations that will be tried on the original dataset as well as the one augmented with the image generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1273,
     "status": "error",
     "timestamp": 1606509307909,
     "user": {
      "displayName": "Mihai Matei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiniPXEs1itGGnxwLSoz5k-dDrj2tfHpef9Lmit=s64",
      "userId": "01318548799329859257"
     },
     "user_tz": -120
    },
    "id": "Lmf4bKO5SnUN",
    "outputId": "9530eb58-decb-46e9-f91d-db4a3073bf19"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = 2 * len(trainDF) / BATCH_SIZE\n",
    "\n",
    "starter_learning_rate = 1e-2\n",
    "end_learning_rate = 5e-3\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    starter_learning_rate,\n",
    "    steps_per_epoch*TRAIN_EPOCHS,\n",
    "    end_learning_rate,\n",
    "    power=0.5)\n",
    "\n",
    "def create_optimizer_sgd(lr=6e-3):\n",
    "    return tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "def create_optimizer_adadelta(lr=1e-3):\n",
    "    return tf.keras.optimizers.Adadelta(lr)\n",
    "\n",
    "\n",
    "def get_class_weights(df):\n",
    "    import sklearn\n",
    "    weights=sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "                                                            classes=range(NO_CLASSES),\n",
    "                                                            y=df['target'])\n",
    "    return {i:weights[i] for i in range(NO_CLASSES)}\n",
    "\n",
    "#optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "\n",
    "\n",
    "#history_aug = model.train_autoencoder(trainDF.append(valDF.append(testDF, ignore_index=True), ignore_index=True))\n",
    "#history_tl = model.train_transfer_learning(trainDF, valDF)\n",
    "\n",
    "\n",
    "# Search the hyperparameter space for the best model\n",
    "train_hyper_lookup = mm.HyperParamsLookup(lambda hyper_params: CNNModel(**hyper_params),\n",
    "                                         lambda hist: np.max(hist.history('accuracy', mm.DataType.VALIDATION)))\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "train_hyper_lookup.grid_search((trainDF, valDF),\n",
    "                               trainEpochs=[TRAIN_EPOCHS],\n",
    "                               conv_dropout=[0.1, 0.2, 0.3],\n",
    "                               dense_dropout=[0.1, 0.3, 0.5],\n",
    "                               augment=[True, False],\n",
    "                               class_weights=[get_class_weights(trainDF), None],\n",
    "                               filter_size=[[64, 128, 256, 512]],\n",
    "                               load_initial_weights=[True],\n",
    "                               optimizer=[create_optimizer_sgd] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4cDfpKjSnUN"
   },
   "source": [
    "##### Hyper parameter search model results\n",
    "Plot the results of the hyperparameter model search.\n",
    "\n",
    "The plots will contain the same color for the same model. The train metrics are ploted using dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwSJiQ2XSnUN"
   },
   "outputs": [],
   "source": [
    "ev = mm.ModelEvaluation(train_hyper_lookup.history)\n",
    "ev.plot_history('Training history', ['accuracy', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNUvRLSNSnUN"
   },
   "source": [
    "##### CNN best model retrained\n",
    "Reload the model using the saved weights in the hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the best model\n",
    "best_history = train_hyper_lookup.best_history\n",
    "hyper_params = best_history.model_params.copy()\n",
    "\n",
    "hyper_params['trainEpochs'] = np.argmax(best_history.history('accuracy', mm.DataType.VALIDATION)) + 1\n",
    "\n",
    "print(\"Best Model using {}\\nvalidation accuracy={}\".format(hyper_params,\n",
    "                                                           max(best_history.history('accuracy', mm.DataType.VALIDATION))))\n",
    "      \n",
    "best_model_train = CNNModel(**hyper_params)\n",
    "best_model_train.load_weights(train_hyper_lookup.best_checkpoint)\n",
    " \n",
    "best_model_train._model.evaluate(TrainDataset(trainDF, img_size=IMAGE_SIZE), return_dict=False)\n",
    "best_model_train._model.evaluate(TrainDataset(valDF, img_size=IMAGE_SIZE), return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP4xRTHGSnUN"
   },
   "source": [
    "#### CNN Model metrics\n",
    "Plot the confusion matrix and Receiver Operating Caracteristic curve for the best model.\n",
    "\n",
    "The ROC curve will be plotted for each class as well as an micro averaged dotted one for all classes.\n",
    "\n",
    "In a binary classification an AUC of 0.5 (blue diagonal line) means that the model has no discriminative capacity to differenciate between the 2 classes.\n",
    "\n",
    "When AUC is 0.7, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "Usually an AUC of 0.8 is considered good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPktHueHSnUN"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix and roc curve for the validation set\n",
    "_, pred_targets, pred_scores = best_model_train.predict(valDF)\n",
    "\n",
    "pb = mm.PlotBuilder().create_confusion_matrix(\n",
    "    valDF['target'], pred_targets, range(0, 8))\n",
    "\n",
    "pb.create_roc_curve_multiclass(\n",
    "    valDF['target'], pred_scores, range(0, 8), [True] * 8).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeK3yaOxSnUN"
   },
   "source": [
    "### Additional training on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "valDFtrain, remainingDFVal = train_test_split(valDF, test_size=0.2,\n",
    "                                              shuffle=True, random_state=int(round(time.time())),\n",
    "                                              stratify=valDF['target'])\n",
    "allDFdata = trainDF.append(valDFtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_coXV3xSnUN"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "hyper_params['trainEpochs'] += 20\n",
    "model_retrain = CNNModel(**hyper_params)\n",
    "history_retrain = model_retrain.train([allDFdata, remainingDFVal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_set_retrain = mm.ModelHistorySet()\n",
    "history_set_retrain.add_history(history_retrain)\n",
    "ev = mm.ModelEvaluation(history_set_retrain)\n",
    "ev.plot_history('Training history', ['accuracy', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgFz3KlbSnUN"
   },
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model again\n",
    "best_hyper_params = model_retrain._hyper_params.copy()\n",
    "best_hyper_params['trainEpochs'] = np.argmax(history_retrain.history('accuracy', mm.DataType.VALIDATION))\n",
    "\n",
    "best_model = CNNModel(**best_hyper_params)\n",
    "best_model.load_weights(model_retrain.checkpoint())\n",
    "\n",
    "#best_model.save_model(name=os.path.join(BEST_MODEL_DIR, \n",
    "#                                        'best_model_{}'.format(datetime.now().strftime(\"%d_%m_%Y_%H_%M\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINED MODEL')\n",
    "best_model_train._model.evaluate(TrainDataset(allDFdata, img_size=IMAGE_SIZE), return_dict=False)\n",
    "best_model_train._model.evaluate(TrainDataset(remainingDFVal, img_size=IMAGE_SIZE), return_dict=False)\n",
    "\n",
    "print('BEST MODEL')\n",
    "best_model._model.evaluate(TrainDataset(allDFdata, img_size=IMAGE_SIZE), return_dict=False)\n",
    "best_model._model.evaluate(TrainDataset(remainingDFVal, img_size=IMAGE_SIZE), return_dict=False)\n",
    "\n",
    "_, pred_targets, pred_scores = best_model.predict(remainingDFVal)\n",
    "pb = mm.PlotBuilder().create_confusion_matrix(\n",
    "    remainingDFVal['target'], pred_targets, range(0, 8))\n",
    "\n",
    "pb.create_roc_curve_multiclass(\n",
    "    remainingDFVal['target'], pred_scores, range(0, 8), [True] * 8).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esMrEhcHSnUN"
   },
   "outputs": [],
   "source": [
    "_, test_target, test_scores = best_model.predict(testDF)\n",
    "\n",
    "pb = mm.PlotBuilder().create_histograms([(test_target, range(0,8))], ['Predicted'])\n",
    "pb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WdT5J3_SnUN"
   },
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPPBpg0ASnUN"
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = f'{OUTPUT_DIR}/Mihai_Matei_submission_{datetime.now().strftime(\"%d_%m_%Y_%H_%M\")}.txt'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i in range(len(test_target)):\n",
    "        f.write(\"{},{}\\n\".format(os.path.basename(testDF['path'].iloc[i]), test_target[i]))\n",
    "        \n",
    "output_path = f'{OUTPUT_DIR}/Mihai_Matei_SCORES_TARGET_{datetime.now().strftime(\"%d_%m_%Y_%H_%M\")}.txt'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i in range(len(test_target)):\n",
    "        f.write(\"{},{} {}\\n\".format(os.path.basename(testDF['path'].iloc[i]), test_target[i], test_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#        inputs_large = tf.keras.Input(shape=LARGE_SIZE + (3,), name=\"input_large\")       \n",
    "    \n",
    "    \n",
    "    # encoder\n",
    "#        layer = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "#        layer = layers.MaxPooling2D((2, 2), padding='same')(layer)\n",
    "#        layer = layers.BatchNormalization()(layer)\n",
    "#        layer = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(layer)\n",
    "#        layer = layers.MaxPooling2D((2, 2), padding='same')(layer)\n",
    "#        layer = layers.BatchNormalization()(layer)\n",
    "#        layer = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(layer)\n",
    "#        layer = layers.Flatten()(layer)\n",
    "#        layer = layers.Dense(64, activation='softmax')(layer)\n",
    "#        encoder = layer\n",
    "#        cnn_part_encoder = encoder\n",
    "            \n",
    "            # decoder\n",
    "#        layer = layers.Reshape((8, 8, 1))(layer)\n",
    "#        layer = layers.Conv2DTranspose(64, (2, 2), activation='relu', padding='same', strides=2)(layer)\n",
    "#        layer = layers.BatchNormalization()(layer)\n",
    "#        layer = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(layer)\n",
    "#        layer = layers.BatchNormalization()(layer)\n",
    "#        layer = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(layer)\n",
    "#        layer = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same',\n",
    "#                              name='DECODER_OUTPUT')(layer)\n",
    "#        decoder = layer\n",
    "            \n",
    "\n",
    "        \n",
    "        #layer = layers.Flatten()(layer)\n",
    "        #layer = layers.Dense(128, kernel_initializer='he_uniform',\n",
    "        #                     kernel_regularizer=tf.keras.regularizers.l2(0.1))(layer)\n",
    "        #layer = layers.BatchNormalization()(layer)\n",
    "        #layer = layers.Activation('relu')(layer)\n",
    "#        layer = layers.Dropout(0.3)(layer)\n",
    "        \n",
    "        # process the encodings of the autoencoder through some dense layer\n",
    "#        encoder = layers.BatchNormalization()(encoder)\n",
    "#        encoder = layers.Dropout(0.3)(encoder)\n",
    "#        encoder = layers.Dense(32, activation='relu', kernel_initializer='he_uniform',\n",
    "#                        kernel_regularizer=tf.keras.regularizers.l2(0.1))(encoder)\n",
    "#        encoder = layers.Dropout(0.5)(encoder)\n",
    "        \n",
    "        # add the transfer learning model\n",
    "        #layer_transfer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
    "        #                                name=\"EFFICIENT_NET\",\n",
    "        #                                input_shape=LARGE_SIZE + (3,), trainable=True)(inputs_large)\n",
    "#        layer_transfer = tf.keras.applications.MobileNetV2(input_shape=LARGE_SIZE + (3,),\n",
    "                                                           #weights='imagenet',\n",
    "#                                                           weights=None,\n",
    "#                                                           include_top=False,\n",
    "#                                                           pooling='avg',\n",
    "#                                                           classes=NO_CLASSES)(inputs_large)\n",
    "#        layer_transfer = layers.BatchNormalization()(layer_transfer)\n",
    "#        layer_transfer = layers.Dropout(0.5)(layer_transfer)\n",
    "#        layer_transfer = layers.Dense(16, kernel_initializer='he_uniform',\n",
    "#                                      kernel_regularizer=tf.keras.regularizers.l2(0.1))(layer_transfer)\n",
    "#        layer_transfer = layers.BatchNormalization()(layer_transfer)\n",
    "#        layer_transfer = layers.Activation('relu')(layer_transfer)\n",
    "#        layer_transfer = layers.Dropout(0.3)(layer_transfer)\n",
    "\n",
    "           # classifier part to be used on initial training\n",
    "#        tf_hub_classifier = layers.BatchNormalization()(layer_transfer)\n",
    "#        tf_hub_classifier = layers.Dropout(0.5)(tf_hub_classifier)\n",
    "#        tf_hub_classifier = layers.Dense(NO_CLASSES, activation='softmax', name=\"TL_OUTPUT\")(tf_hub_classifier)\n",
    "        \n",
    "        \n",
    "        #layer = layers.Dense(256, kernel_initializer='he_uniform',\n",
    "        #                              kernel_regularizer=tf.keras.regularizers.l2(0.1))(layer)\n",
    "        #layer = layers.BatchNormalization()(layer)\n",
    "        #layer = layers.Activation('relu')(layer)\n",
    "        \n",
    "        \n",
    "        #layer = tfp.layers.DenseFlipout(32,\n",
    "        #                        bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(), name=\"VIlayer\")(layer)\n",
    "        \n",
    "        # concatenate the encoder and classfier and transfer leaning layers\n",
    "        #layer = layers.Concatenate()([encoder, layer])\n",
    "        #layer = layers.BatchNormalization()(layer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "competition_cv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
